use std::borrow::Cow;
use std::collections::hash_map::DefaultHasher;
use std::collections::HashSet;
use std::fmt::Write;
use std::hash::{Hash, Hasher};

use ahash::AHashSet;

use crate::args::ArgValues;
use crate::resource::{ResourceError, ResourceTracker};
use crate::run::RunResult;
use crate::value::{Attr, Value};
use crate::values::PyTrait;
use crate::values::{Bytes, Dict, List, Str, Tuple};

/// Unique identifier for values stored inside the heap arena.
pub type HeapId = usize;

/// HeapData captures every runtime value that must live in the arena.
///
/// Each variant wraps a type that implements `AbstractValue`, providing
/// Python-compatible operations. The trait is manually implemented to dispatch
/// to the appropriate variant's implementation.
///
/// Note: The `Value` variant is special - it wraps boxed immediate values
/// that need heap identity (e.g., when `id()` is called on an int).
#[derive(Debug)]
pub enum HeapData<'c, 'e> {
    Str(Str),
    Bytes(Bytes),
    List(List<'c, 'e>),
    Tuple(Tuple<'c, 'e>),
    Dict(Dict<'c, 'e>),
    /// A cell wrapping a single mutable value for closure support.
    ///
    /// Cells enable nonlocal variable access by providing a heap-allocated
    /// container that can be shared between a function and its nested functions.
    /// Both the outer function and inner function hold references to the same
    /// cell, allowing modifications to propagate across scope boundaries.
    Cell(Value<'c, 'e>),
    // TODO: support arbitrary classes
}

impl<'c, 'e> HeapData<'c, 'e> {
    /// Computes hash for immutable heap types that can be used as dict keys.
    ///
    /// Returns Some(hash) for immutable types (Str, Bytes, Tuple of hashables).
    /// Returns None for mutable types (List, Dict) which cannot be dict keys.
    ///
    /// This is called lazily when the value is first used as a dict key,
    /// avoiding unnecessary hash computation for values that are never used as keys.
    fn compute_hash_if_immutable<T: ResourceTracker>(&self, heap: &mut Heap<'c, 'e, T>) -> Option<u64> {
        match self {
            Self::Str(s) => {
                let mut hasher = DefaultHasher::new();
                s.as_str().hash(&mut hasher);
                Some(hasher.finish())
            }
            Self::Bytes(b) => {
                let mut hasher = DefaultHasher::new();
                b.as_slice().hash(&mut hasher);
                Some(hasher.finish())
            }
            Self::Tuple(t) => {
                // Tuple is hashable only if all elements are hashable
                let mut hasher = DefaultHasher::new();
                for obj in t.as_vec() {
                    match obj.py_hash_u64(heap) {
                        Some(h) => h.hash(&mut hasher),
                        None => return None, // Contains unhashable element
                    }
                }
                Some(hasher.finish())
            }
            // Mutable types cannot be hashed (Cell is handled specially in get_or_compute_hash)
            Self::List(_) | Self::Dict(_) | Self::Cell(_) => None,
        }
    }
}

/// Manual implementation of AbstractValue dispatch for HeapData.
///
/// This provides efficient dispatch without boxing overhead by matching on
/// the enum variant and delegating to the inner type's implementation.
impl<'c, 'e> PyTrait<'c, 'e> for HeapData<'c, 'e> {
    fn py_type<T: ResourceTracker>(&self, heap: Option<&Heap<'c, 'e, T>>) -> &'static str {
        match self {
            Self::Str(s) => s.py_type(heap),
            Self::Bytes(b) => b.py_type(heap),
            Self::List(l) => l.py_type(heap),
            Self::Tuple(t) => t.py_type(heap),
            Self::Dict(d) => d.py_type(heap),
            Self::Cell(_) => "cell",
        }
    }

    fn py_estimate_size(&self) -> usize {
        match self {
            Self::Str(s) => s.py_estimate_size(),
            Self::Bytes(b) => b.py_estimate_size(),
            Self::List(l) => l.py_estimate_size(),
            Self::Tuple(t) => t.py_estimate_size(),
            Self::Dict(d) => d.py_estimate_size(),
            Self::Cell(v) => std::mem::size_of::<Value>() + v.py_estimate_size(),
        }
    }

    fn py_len<T: ResourceTracker>(&self, heap: &Heap<'c, 'e, T>) -> Option<usize> {
        match self {
            Self::Str(s) => PyTrait::py_len(s, heap),
            Self::Bytes(b) => PyTrait::py_len(b, heap),
            Self::List(l) => PyTrait::py_len(l, heap),
            Self::Tuple(t) => PyTrait::py_len(t, heap),
            Self::Dict(d) => PyTrait::py_len(d, heap),
            Self::Cell(_) => None, // Cells don't have length
        }
    }

    fn py_eq<T: ResourceTracker>(&self, other: &Self, heap: &mut Heap<'c, 'e, T>) -> bool {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_eq(b, heap),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_eq(b, heap),
            (Self::List(a), Self::List(b)) => a.py_eq(b, heap),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_eq(b, heap),
            (Self::Dict(a), Self::Dict(b)) => a.py_eq(b, heap),
            // Cells compare by identity only (handled at Value level via HeapId comparison)
            (Self::Cell(_), Self::Cell(_)) => false,
            _ => false, // Different types are never equal
        }
    }

    fn py_dec_ref_ids(&mut self, stack: &mut Vec<HeapId>) {
        match self {
            Self::Str(s) => s.py_dec_ref_ids(stack),
            Self::Bytes(b) => b.py_dec_ref_ids(stack),
            Self::List(l) => l.py_dec_ref_ids(stack),
            Self::Tuple(t) => t.py_dec_ref_ids(stack),
            Self::Dict(d) => d.py_dec_ref_ids(stack),
            Self::Cell(v) => v.py_dec_ref_ids(stack),
        }
    }

    fn py_bool<T: ResourceTracker>(&self, heap: &Heap<'c, 'e, T>) -> bool {
        match self {
            Self::Str(s) => s.py_bool(heap),
            Self::Bytes(b) => b.py_bool(heap),
            Self::List(l) => l.py_bool(heap),
            Self::Tuple(t) => t.py_bool(heap),
            Self::Dict(d) => d.py_bool(heap),
            Self::Cell(_) => true, // Cells are always truthy
        }
    }

    fn py_repr_fmt<W: Write, T: ResourceTracker>(
        &self,
        f: &mut W,
        heap: &Heap<'c, 'e, T>,
        heap_ids: &mut AHashSet<usize>,
    ) -> std::fmt::Result {
        match self {
            Self::Str(s) => s.py_repr_fmt(f, heap, heap_ids),
            Self::Bytes(b) => b.py_repr_fmt(f, heap, heap_ids),
            Self::List(l) => l.py_repr_fmt(f, heap, heap_ids),
            Self::Tuple(t) => t.py_repr_fmt(f, heap, heap_ids),
            Self::Dict(d) => d.py_repr_fmt(f, heap, heap_ids),
            // Cell repr shows the contained value's type
            Self::Cell(v) => write!(f, "<cell: {} object>", v.py_type(Some(heap))),
        }
    }

    fn py_str<T: ResourceTracker>(&self, heap: &Heap<'c, 'e, T>) -> Cow<'static, str> {
        match self {
            Self::Str(s) => s.py_str(heap),
            Self::Bytes(b) => b.py_str(heap),
            Self::List(l) => l.py_str(heap),
            Self::Tuple(t) => t.py_str(heap),
            Self::Dict(d) => d.py_str(heap),
            Self::Cell(v) => format!("<cell: {} object>", v.py_type(Some(heap))).into(),
        }
    }

    fn py_add<T: ResourceTracker>(
        &self,
        other: &Self,
        heap: &mut Heap<'c, 'e, T>,
    ) -> Result<Option<Value<'c, 'e>>, crate::resource::ResourceError> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_add(b, heap),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_add(b, heap),
            (Self::List(a), Self::List(b)) => a.py_add(b, heap),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_add(b, heap),
            (Self::Dict(a), Self::Dict(b)) => a.py_add(b, heap),
            // Cells don't support arithmetic operations
            _ => Ok(None),
        }
    }

    fn py_sub<T: ResourceTracker>(
        &self,
        other: &Self,
        heap: &mut Heap<'c, 'e, T>,
    ) -> Result<Option<Value<'c, 'e>>, crate::resource::ResourceError> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_sub(b, heap),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_sub(b, heap),
            (Self::List(a), Self::List(b)) => a.py_sub(b, heap),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_sub(b, heap),
            (Self::Dict(a), Self::Dict(b)) => a.py_sub(b, heap),
            // Cells don't support arithmetic operations
            _ => Ok(None),
        }
    }

    fn py_mod(&self, other: &Self) -> Option<Value<'c, 'e>> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_mod(b),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_mod(b),
            (Self::List(a), Self::List(b)) => a.py_mod(b),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_mod(b),
            (Self::Dict(a), Self::Dict(b)) => a.py_mod(b),
            // Cells don't support arithmetic operations
            _ => None,
        }
    }

    fn py_mod_eq(&self, other: &Self, right_value: i64) -> Option<bool> {
        match (self, other) {
            (Self::Str(a), Self::Str(b)) => a.py_mod_eq(b, right_value),
            (Self::Bytes(a), Self::Bytes(b)) => a.py_mod_eq(b, right_value),
            (Self::List(a), Self::List(b)) => a.py_mod_eq(b, right_value),
            (Self::Tuple(a), Self::Tuple(b)) => a.py_mod_eq(b, right_value),
            (Self::Dict(a), Self::Dict(b)) => a.py_mod_eq(b, right_value),
            // Cells don't support arithmetic operations
            _ => None,
        }
    }

    fn py_iadd<T: ResourceTracker>(
        &mut self,
        other: Value<'c, 'e>,
        heap: &mut Heap<'c, 'e, T>,
        self_id: Option<HeapId>,
    ) -> Result<bool, crate::resource::ResourceError> {
        match self {
            Self::Str(s) => s.py_iadd(other, heap, self_id),
            Self::Bytes(b) => b.py_iadd(other, heap, self_id),
            Self::List(l) => l.py_iadd(other, heap, self_id),
            Self::Tuple(t) => t.py_iadd(other, heap, self_id),
            Self::Dict(d) => d.py_iadd(other, heap, self_id),
            Self::Cell(_) => Ok(false), // Cells don't support in-place add
        }
    }

    fn py_call_attr<T: ResourceTracker>(
        &mut self,
        heap: &mut Heap<'c, 'e, T>,
        attr: &Attr,
        args: ArgValues<'c, 'e>,
    ) -> RunResult<'c, Value<'c, 'e>> {
        match self {
            Self::Str(s) => s.py_call_attr(heap, attr, args),
            Self::Bytes(b) => b.py_call_attr(heap, attr, args),
            Self::List(l) => l.py_call_attr(heap, attr, args),
            Self::Tuple(t) => t.py_call_attr(heap, attr, args),
            Self::Dict(d) => d.py_call_attr(heap, attr, args),
            Self::Cell(_) => Err(crate::exceptions::ExcType::attribute_error("cell", attr)),
        }
    }

    fn py_getitem<T: ResourceTracker>(
        &self,
        key: &Value<'c, 'e>,
        heap: &mut Heap<'c, 'e, T>,
    ) -> RunResult<'c, Value<'c, 'e>> {
        match self {
            Self::Str(s) => s.py_getitem(key, heap),
            Self::Bytes(b) => b.py_getitem(key, heap),
            Self::List(l) => l.py_getitem(key, heap),
            Self::Tuple(t) => t.py_getitem(key, heap),
            Self::Dict(d) => d.py_getitem(key, heap),
            Self::Cell(_) => Err(crate::exceptions::ExcType::type_error_not_sub("cell")),
        }
    }

    fn py_setitem<T: ResourceTracker>(
        &mut self,
        key: Value<'c, 'e>,
        value: Value<'c, 'e>,
        heap: &mut Heap<'c, 'e, T>,
    ) -> RunResult<'c, ()> {
        match self {
            Self::Str(s) => s.py_setitem(key, value, heap),
            Self::Bytes(b) => b.py_setitem(key, value, heap),
            Self::List(l) => l.py_setitem(key, value, heap),
            Self::Tuple(t) => t.py_setitem(key, value, heap),
            Self::Dict(d) => d.py_setitem(key, value, heap),
            Self::Cell(_) => Err(crate::exceptions::ExcType::type_error_not_sub_assignment("cell")),
        }
    }
}

/// Hash caching state stored alongside each heap entry.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum HashState {
    /// Hash has not yet been computed but the value might be hashable.
    Unknown,
    /// Cached hash value for immutable types that have been hashed at least once.
    Cached(u64),
    /// Value is unhashable (mutable types or tuples containing unhashables).
    Unhashable,
}

impl HashState {
    fn for_data(data: &HeapData<'_, '_>) -> Self {
        match data {
            // Cells are hashable by identity (like all Python objects without __hash__ override)
            HeapData::Str(_) | HeapData::Bytes(_) | HeapData::Tuple(_) | HeapData::Cell(_) => Self::Unknown,
            HeapData::List(_) | HeapData::Dict(_) => Self::Unhashable,
        }
    }
}

/// A single entry inside the heap arena, storing refcount, payload, and hash metadata.
///
/// The `hash_state` field tracks whether the heap entry is hashable and, if so,
/// caches the computed hash. Mutable types (List, Dict) start as `Unhashable` and
/// will raise TypeError if used as dict keys.
///
/// The `data` field is an Option to support temporary borrowing: when methods like
/// `with_entry_mut` or `call_attr` need mutable access to both the data and the heap,
/// they can `.take()` the data out (leaving `None`), pass `&mut Heap` to user code,
/// then restore the data. This avoids unsafe code while keeping `refcount` accessible
/// for `inc_ref`/`dec_ref` during the borrow.
#[derive(Debug)]
struct HeapValue<'c, 'e> {
    refcount: usize,
    /// The payload data. Temporarily `None` while borrowed via `with_entry_mut`/`call_attr`.
    data: Option<HeapData<'c, 'e>>,
    /// Current hashing status / cached hash value
    hash_state: HashState,
}

/// Reference-counted arena that backs all heap-only runtime values.
///
/// Uses a free list to reuse slots from freed values, keeping memory usage
/// constant for long-running loops that repeatedly allocate and free values.
/// When an value is freed via `dec_ref`, its slot ID is added to the free list.
/// New allocations pop from the free list when available, otherwise append.
///
/// Generic over `T: ResourceTracker` to support different resource tracking strategies.
/// When `T = NoLimitTracker` (the default), all resource checks compile away to no-ops.
#[derive(Debug)]
pub struct Heap<'c, 'e, T: ResourceTracker> {
    entries: Vec<Option<HeapValue<'c, 'e>>>,
    /// IDs of freed slots available for reuse. Populated by `dec_ref`, consumed by `allocate`.
    free_list: Vec<HeapId>,
    /// Resource tracker for enforcing limits and scheduling GC.
    tracker: T,
}

macro_rules! take_data {
    ($self:ident, $id:expr, $func_name:literal) => {
        $self
            .entries
            .get_mut($id)
            .expect(concat!("Heap::", $func_name, ": slot missing"))
            .as_mut()
            .expect(concat!("Heap::", $func_name, ": object already freed"))
            .data
            .take()
            .expect(concat!("Heap::", $func_name, ": data already borrowed"))
    };
}

macro_rules! restore_data {
    ($self:ident, $id:expr, $new_data:expr, $func_name:literal) => {{
        let entry = $self
            .entries
            .get_mut($id)
            .expect(concat!("Heap::", $func_name, ": slot missing"))
            .as_mut()
            .expect(concat!("Heap::", $func_name, ": object already freed"));
        entry.data = Some($new_data);
    }};
}

impl<'c, 'e, T: ResourceTracker> Heap<'c, 'e, T> {
    /// Creates a new heap with the given resource tracker.
    ///
    /// Use this to create heaps with custom resource limits or GC scheduling.
    pub fn new(capacity: usize, tracker: T) -> Self {
        Self {
            entries: Vec::with_capacity(capacity),
            free_list: Vec::new(),
            tracker,
        }
    }

    /// Returns a reference to the resource tracker.
    pub fn tracker(&self) -> &T {
        &self.tracker
    }

    /// Returns a mutable reference to the resource tracker.
    pub fn tracker_mut(&mut self) -> &mut T {
        &mut self.tracker
    }

    /// Allocates a new heap entry.
    ///
    /// Returns `Err(ResourceError)` if allocation would exceed configured limits.
    /// Use this when you need to handle resource limit errors gracefully.
    pub fn allocate(&mut self, data: HeapData<'c, 'e>) -> Result<HeapId, ResourceError> {
        self.tracker.on_allocate(|| data.py_estimate_size())?;

        let hash_state = HashState::for_data(&data);
        let new_entry = HeapValue {
            refcount: 1,
            data: Some(data),
            hash_state,
        };

        let id = if let Some(id) = self.free_list.pop() {
            // Reuse a freed slot
            self.entries[id] = Some(new_entry);
            id
        } else {
            // No free slots, append new entry
            let id = self.entries.len();
            self.entries.push(Some(new_entry));
            id
        };

        Ok(id)
    }

    /// Allocates a new cell containing the given value.
    ///
    /// Cells are used for closure support, allowing values to be shared between
    /// a function and its nested closures. The cell is created with refcount 1.
    ///
    /// Note: The contained value's refcount is NOT incremented. The caller is
    /// responsible for ensuring proper reference counting of the value before
    /// putting it in a cell (typically by cloning with `clone_with_heap`).
    pub fn alloc_cell(&mut self, value: Value<'c, 'e>) -> HeapId {
        // Cell allocation is considered infallible - cells are small and essential
        // for closure support. Panic if allocation fails (should be rare).
        self.allocate(HeapData::Cell(value))
            .expect("cell allocation failed - out of resources")
    }

    /// Increments the reference count for an existing heap entry.
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    pub fn inc_ref(&mut self, id: HeapId) {
        let value = self
            .entries
            .get_mut(id)
            .expect("Heap::inc_ref: slot missing")
            .as_mut()
            .expect("Heap::inc_ref: object already freed");
        value.refcount += 1;
    }

    /// Decrements the reference count and frees the value (plus children) once it hits zero.
    ///
    /// When an value is freed, its slot ID is added to the free list for reuse by
    /// future allocations. Uses recursion for child cleanup - avoiding repeated Vec
    /// allocations and benefiting from call stack locality.
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    pub fn dec_ref(&mut self, id: HeapId) {
        let slot = self.entries.get_mut(id).expect("Heap::dec_ref: slot missing");
        let entry = slot.as_mut().expect("Heap::dec_ref: object already freed");
        if entry.refcount > 1 {
            entry.refcount -= 1;
        } else if let Some(value) = slot.take() {
            // refcount == 1, free the value and add slot to free list for reuse
            self.free_list.push(id);

            // Notify tracker of freed memory
            if let Some(ref data) = value.data {
                self.tracker.on_free(|| data.py_estimate_size());
            }

            // Collect child IDs and mark Values as Dereferenced (when dec-ref-check enabled)
            if let Some(mut data) = value.data {
                let mut child_ids = Vec::new();
                data.py_dec_ref_ids(&mut child_ids);
                drop(data);
                // Recursively decrement children
                for child_id in child_ids {
                    self.dec_ref(child_id);
                }
            }
        }
    }

    /// Returns an immutable reference to the heap data stored at the given ID.
    ///
    /// # Panics
    /// Panics if the value ID is invalid, the value has already been freed,
    /// or the data is currently borrowed via `with_entry_mut`/`call_attr`.
    #[must_use]
    pub fn get(&self, id: HeapId) -> &HeapData<'c, 'e> {
        self.entries
            .get(id)
            .expect("Heap::get: slot missing")
            .as_ref()
            .expect("Heap::get: object already freed")
            .data
            .as_ref()
            .expect("Heap::get: data currently borrowed")
    }

    /// Returns a mutable reference to the heap data stored at the given ID.
    ///
    /// # Panics
    /// Panics if the value ID is invalid, the value has already been freed,
    /// or the data is currently borrowed via `with_entry_mut`/`call_attr`.
    pub fn get_mut(&mut self, id: HeapId) -> &mut HeapData<'c, 'e> {
        self.entries
            .get_mut(id)
            .expect("Heap::get_mut: slot missing")
            .as_mut()
            .expect("Heap::get_mut: object already freed")
            .data
            .as_mut()
            .expect("Heap::get_mut: data currently borrowed")
    }

    /// Returns or computes the hash for the heap entry at the given ID.
    ///
    /// Hashes are computed lazily on first use and then cached. Returns
    /// Some(hash) for immutable types (Str, Bytes, hashable Tuple), None
    /// for mutable types (List, Dict).
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    pub fn get_or_compute_hash(&mut self, id: HeapId) -> Option<u64> {
        let entry = self
            .entries
            .get_mut(id)
            .expect("Heap::get_or_compute_hash: slot missing")
            .as_mut()
            .expect("Heap::get_or_compute_hash: object already freed");

        match entry.hash_state {
            HashState::Unhashable => return None,
            HashState::Cached(hash) => return Some(hash),
            HashState::Unknown => {}
        }

        // Handle Cell specially - uses identity-based hashing (like Python cell objects)
        if let Some(HeapData::Cell(_)) = &entry.data {
            let mut hasher = DefaultHasher::new();
            id.hash(&mut hasher);
            let hash = hasher.finish();
            entry.hash_state = HashState::Cached(hash);
            return Some(hash);
        }

        // Compute hash lazily - need to temporarily take data to avoid borrow conflict
        let data = entry.data.take().expect("Heap::get_or_compute_hash: data borrowed");
        let hash = data.compute_hash_if_immutable(self);

        // Restore data and cache the hash if computed
        let entry = self
            .entries
            .get_mut(id)
            .expect("Heap::get_or_compute_hash: slot missing after compute")
            .as_mut()
            .expect("Heap::get_or_compute_hash: object freed during compute");
        entry.data = Some(data);
        entry.hash_state = match hash {
            Some(value) => HashState::Cached(value),
            None => HashState::Unhashable,
        };
        hash
    }

    /// Calls an attribute on the heap entry at `id` while temporarily taking ownership
    /// of its payload so we can borrow the heap again inside the call. This avoids the
    /// borrow checker conflict that arises when attribute implementations also need
    /// mutable access to the heap (e.g. for refcounting).
    pub fn call_attr(&mut self, id: HeapId, attr: &Attr, args: ArgValues<'c, 'e>) -> RunResult<'c, Value<'c, 'e>> {
        // Take data out in a block so the borrow of self.entries ends
        let mut data = take_data!(self, id, "call_attr");

        let result = data.py_call_attr(self, attr, args);

        // Restore data
        let entry = self
            .entries
            .get_mut(id)
            .expect("Heap::call_attr: slot missing")
            .as_mut()
            .expect("Heap::call_attr: object already freed");
        entry.data = Some(data);
        result
    }

    /// Gives mutable access to a heap entry while allowing reentrant heap usage
    /// inside the closure (e.g. to read other values or allocate results).
    ///
    /// The data is temporarily taken from the heap entry, so the closure can safely
    /// mutate both the entry data and the heap (e.g. to allocate new values).
    /// The data is automatically restored after the closure completes.
    pub fn with_entry_mut<F, R>(&mut self, id: HeapId, f: F) -> R
    where
        F: FnOnce(&mut Heap<'c, 'e, T>, &mut HeapData<'c, 'e>) -> R,
    {
        // Take data out in a block so the borrow of self.entries ends
        let mut data = take_data!(self, id, "with_entry_mut");

        let result = f(self, &mut data);

        // Restore data
        restore_data!(self, id, data, "with_entry_mut");
        result
    }

    /// Temporarily takes ownership of two heap entries so their data can be borrowed
    /// simultaneously while still permitting mutable access to the heap (e.g. to
    /// allocate results). Automatically restores both entries after the closure
    /// finishes executing.
    pub fn with_two<F, R>(&mut self, left: HeapId, right: HeapId, f: F) -> R
    where
        F: FnOnce(&mut Heap<'c, 'e, T>, &HeapData<'c, 'e>, &HeapData<'c, 'e>) -> R,
    {
        if left == right {
            // Same value - take data once and pass it twice
            let data = take_data!(self, left, "with_two");

            let result = f(self, &data, &data);

            restore_data!(self, left, data, "with_two");
            result
        } else {
            // Different values - take both
            let left_data = take_data!(self, left, "with_two (left)");
            let right_data = take_data!(self, right, "with_two (right)");

            let result = f(self, &left_data, &right_data);

            // Restore in reverse order
            restore_data!(self, right, right_data, "with_two (right)");
            restore_data!(self, left, left_data, "with_two (left)");
            result
        }
    }

    /// Removes all values and resets the ID counter, used between executor runs.
    pub fn clear(&mut self) {
        // When dec-ref-check is enabled, mark all contained Values as Dereferenced
        // before clearing to prevent Drop panics. We use py_dec_ref_ids for this
        // since it handles the marking (we ignore the collected IDs since we're
        // clearing everything anyway).
        #[cfg(feature = "dec-ref-check")]
        {
            let mut dummy_stack = Vec::new();
            for value in self.entries.iter_mut().flatten() {
                if let Some(data) = &mut value.data {
                    data.py_dec_ref_ids(&mut dummy_stack);
                }
            }
        }
        self.entries.clear();
        self.free_list.clear();
    }

    /// Returns the reference count for the heap entry at the given ID.
    ///
    /// This is primarily used for testing reference counting behavior.
    ///
    /// # Panics
    /// Panics if the value ID is invalid or the value has already been freed.
    #[must_use]
    pub fn get_refcount(&self, id: HeapId) -> usize {
        self.entries
            .get(id)
            .expect("Heap::get_refcount: slot missing")
            .as_ref()
            .expect("Heap::get_refcount: object already freed")
            .refcount
    }

    /// Returns the number of live (non-freed) values on the heap.
    ///
    /// This is primarily used for testing to verify that all heap entries
    /// are accounted for in reference count tests.
    #[must_use]
    pub fn entry_count(&self) -> usize {
        self.entries.iter().filter(|o| o.is_some()).count()
    }

    /// Gets the value inside a cell, cloning it with proper refcount handling.
    ///
    /// Uses `clone_with_heap` to properly handle all value types including closures,
    /// which need their captured cell refcounts incremented.
    ///
    /// # Panics
    /// Panics if the ID is invalid, the value has been freed, or the entry is not a Cell.
    pub fn get_cell_value(&mut self, id: HeapId) -> Value<'c, 'e> {
        // Take the data out to avoid borrow conflicts when cloning
        let data = take_data!(self, id, "get_cell_value");

        let result = match &data {
            HeapData::Cell(v) => v.clone_with_heap(self),
            _ => panic!("Heap::get_cell_value: entry is not a Cell"),
        };

        // Restore data before returning
        restore_data!(self, id, data, "get_cell_value");

        result
    }

    /// Sets the value inside a cell, properly dropping the old value.
    ///
    /// # Panics
    /// Panics if the ID is invalid, the value has been freed, or the entry is not a Cell.
    pub fn set_cell_value(&mut self, id: HeapId, value: Value<'c, 'e>) {
        // Take the data out to avoid borrow conflicts
        let mut data = take_data!(self, id, "set_cell_value");

        match &mut data {
            HeapData::Cell(old_value) => {
                // Swap in the new value
                let old = std::mem::replace(old_value, value);
                // Restore data first, then drop old value
                restore_data!(self, id, data, "set_cell_value");
                old.drop_with_heap(self);
            }
            _ => panic!("Heap::set_cell_value: entry is not a Cell"),
        }
    }

    /// Returns a reference to the value inside a cell without cloning.
    ///
    /// Useful when you only need to read the cell's value temporarily.
    ///
    /// # Panics
    /// Panics if the ID is invalid, the value has been freed, or the entry is not a Cell.
    pub fn get_cell_value_ref(&self, id: HeapId) -> &Value<'c, 'e> {
        let data = self.get(id);
        match data {
            HeapData::Cell(v) => v,
            _ => panic!("Heap::get_cell_value_ref: entry is not a Cell"),
        }
    }

    /// Helper for List in-place add: extends the destination vec with items from a heap list.
    ///
    /// This method exists to work around borrow checker limitations when List::py_iadd
    /// needs to read from one heap entry while extending another. By keeping both
    /// the read and the refcount increments within Heap's impl block, we can use the
    /// take/restore pattern to avoid the lifetime propagation issues.
    ///
    /// Returns `true` if successful, `false` if the source ID is not a List.
    pub fn iadd_extend_list(&mut self, source_id: HeapId, dest: &mut Vec<Value<'c, 'e>>) -> bool {
        // Take the source data temporarily
        let source_data = take_data!(self, source_id, "iadd_extend_list");

        let success = if let HeapData::List(list) = &source_data {
            // Copy items and track which refs need incrementing
            let items: Vec<Value<'c, 'e>> = list.as_vec().iter().map(Value::copy_for_extend).collect();
            let ref_ids: Vec<HeapId> = items
                .iter()
                .filter_map(|obj| if let Value::Ref(id) = obj { Some(*id) } else { None })
                .collect();

            // Restore source data before mutating heap (inc_ref needs it)
            restore_data!(self, source_id, source_data, "iadd_extend_list");

            // Now increment refcounts
            for id in ref_ids {
                self.inc_ref(id);
            }

            // Extend destination
            dest.extend(items);
            true
        } else {
            // Not a list, restore and return false
            restore_data!(self, source_id, source_data, "iadd_extend_list");
            false
        };

        success
    }

    /// Runs mark-sweep garbage collection to free unreachable cycles.
    ///
    /// This method takes a closure that provides an iterator of root HeapIds
    /// (typically from Namespaces). It marks all reachable objects starting
    /// from roots, then sweeps (frees) any unreachable objects.
    ///
    /// This is necessary because reference counting alone cannot free cycles
    /// where objects reference each other but are unreachable from the program.
    ///
    /// # Arguments
    /// * `get_roots` - Closure returning an iterator of HeapIds that are roots
    pub fn collect_garbage<I, F>(&mut self, get_roots: F)
    where
        I: Iterator<Item = HeapId>,
        F: FnOnce() -> I,
    {
        // Mark phase: collect all reachable IDs using BFS
        let mut reachable: HashSet<HeapId> = HashSet::new();
        let mut work_list: Vec<HeapId> = get_roots().collect();

        while let Some(id) = work_list.pop() {
            if !reachable.insert(id) {
                continue; // Already visited
            }

            // Add children to work list
            if let Some(Some(entry)) = self.entries.get(id) {
                if let Some(ref data) = entry.data {
                    self.collect_child_ids(data, &mut work_list);
                }
            }
        }

        // Sweep phase: free unreachable values
        for (id, value) in self.entries.iter_mut().enumerate() {
            if reachable.contains(&id) {
                continue;
            }

            // This entry is unreachable - free it
            if let Some(value) = value.take() {
                // Notify tracker of freed memory
                if let Some(ref data) = value.data {
                    self.tracker.on_free(|| data.py_estimate_size());
                }

                self.free_list.push(id);

                // Mark Values as Dereferenced when dec-ref-check is enabled
                #[cfg(feature = "dec-ref-check")]
                if let Some(mut data) = value.data {
                    data.py_dec_ref_ids(&mut Vec::new());
                }
            }
        }

        // Notify tracker that GC is complete
        self.tracker.on_gc_complete();
    }

    /// Collects child HeapIds from a HeapData value for GC traversal.
    fn collect_child_ids(&self, data: &HeapData<'c, 'e>, work_list: &mut Vec<HeapId>) {
        match data {
            HeapData::Str(_) | HeapData::Bytes(_) => {}
            HeapData::List(list) => {
                for value in list.as_vec() {
                    if let Value::Ref(id) = value {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Tuple(tuple) => {
                for value in tuple.as_vec() {
                    if let Value::Ref(id) = value {
                        work_list.push(*id);
                    }
                }
            }
            HeapData::Dict(dict) => {
                // Iterate through all hash buckets and their (key, value) pairs
                for bucket in dict.as_index_map().values() {
                    for (k, v) in bucket {
                        if let Value::Ref(id) = k {
                            work_list.push(*id);
                        }
                        if let Value::Ref(id) = v {
                            work_list.push(*id);
                        }
                    }
                }
            }
            HeapData::Cell(value) => {
                // Cell can contain a reference to another heap value
                if let Value::Ref(id) = value {
                    work_list.push(*id);
                }
            }
        }
    }
}

/// Drop implementation for Heap that marks all contained Objects as Dereferenced
/// before dropping to prevent panics when the `dec-ref-check` feature is enabled.
#[cfg(feature = "dec-ref-check")]
impl<T: ResourceTracker> Drop for Heap<'_, '_, T> {
    fn drop(&mut self) {
        // Mark all contained Objects as Dereferenced before dropping.
        // We use py_dec_ref_ids for this since it handles the marking
        // (we ignore the collected IDs since we're dropping everything anyway).
        let mut dummy_stack = Vec::new();
        for value in self.entries.iter_mut().flatten() {
            if let Some(data) = &mut value.data {
                data.py_dec_ref_ids(&mut dummy_stack);
            }
        }
    }
}
